{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofrSTReRco-d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4XbDAIYkrmN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the XLSX file\n",
        "df = pd.read_excel('CL-II-MisinformationData.xlsx')\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv('CL-II-MisinformationData.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIvZGN0Ok-A-",
        "outputId": "a11a5ff5-7a6c-4b88-dd2e-2b06b5bb6224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/CL-II-MisinformationData.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-181319645e33>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load your CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/CL-II-MisinformationData.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your CSV file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Split into train (80%) and temp (20%)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/CL-II-MisinformationData.csv'"
          ]
        }
      ],
      "source": [
        "# task1_prepare_dataset.py\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv('/content/CL-II-MisinformationData.csv')  # Replace with your CSV file name\n",
        "\n",
        "# Split into train (80%) and temp (20%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "# Split temp into validation (10%) and test (10%)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, shuffle=True, random_state=42)\n",
        "\n",
        "# Save splits\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "val_df.to_csv('val.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5UWPTPh9rHLG",
        "outputId": "7a927b2d-08a6-4e05-9bfd-114c938f1204"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 8480,\n  \"fields\": [\n    {\n      \"column\": \"tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8479,\n        \"samples\": [\n          \"#Parents: Be prepared to discuss #COVID19 with your children. Watch this new video for tips on how to support your children during the COVID-19 outbreak. https://t.co/GKZV8vbeel https://t.co/fHmcB7nInb\",\n          \"RT @CDCemergency: Handwashing is one of the best ways to protect yourself &amp; your family from getting sick with #COVID19. Learn when &amp; how y\\u2026\",\n          \"Woman burned her hands while putting on the stove due to high alcohol content in hand sanitiser\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"fake\",\n          \"real\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1f626069-d68a-4c31-882c-662ca6b8e716\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#IndiaFightsCorona: India reports more than 90...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The continuing good news is that daily reporte...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>• 3 people are currently in hospital one each ...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>On the 15/03 NCDC directly contacted a Twitter...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>India’s confirmed case count crosses ten lakh ...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f626069-d68a-4c31-882c-662ca6b8e716')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1f626069-d68a-4c31-882c-662ca6b8e716 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1f626069-d68a-4c31-882c-662ca6b8e716');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2f4e0188-98ff-480e-9cb3-42fb500cfe42\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2f4e0188-98ff-480e-9cb3-42fb500cfe42')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2f4e0188-98ff-480e-9cb3-42fb500cfe42 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               tweet label\n",
              "0  #IndiaFightsCorona: India reports more than 90...  real\n",
              "1  The continuing good news is that daily reporte...  real\n",
              "2  • 3 people are currently in hospital one each ...  real\n",
              "3  On the 15/03 NCDC directly contacted a Twitter...  real\n",
              "4  India’s confirmed case count crosses ten lakh ...  fake"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('/content/train.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWJF4eRZq7wO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "  # Lowercase the text\n",
        "  text = text.lower()\n",
        "  # Remove Special Characters and Numbers\n",
        "  text = re.sub(r'[^a-zA-Z\\s]','', text)\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text\n",
        "\n",
        "# Load the trianing data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Apply preprocessing to the 'tweet' column\n",
        "train_df['processed_tweet'] = train_df['tweet'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO1Uw5bBXmrn",
        "outputId": "7489f911-179f-4d10-e76a-34f5d09b34ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               tweet label  \\\n",
            "0  #IndiaFightsCorona: India reports more than 90...  real   \n",
            "1  The continuing good news is that daily reporte...  real   \n",
            "2  • 3 people are currently in hospital one each ...  real   \n",
            "3  On the 15/03 NCDC directly contacted a Twitter...  real   \n",
            "4  India’s confirmed case count crosses ten lakh ...  fake   \n",
            "\n",
            "                                     processed_tweet  aajtak  aamaadmiparty  \\\n",
            "0  indiafightscorona india reports more than reco...     0.0            0.0   \n",
            "1  the continuing good news is that daily reporte...     0.0            0.0   \n",
            "2  people are currently in hospital one each in a...     0.0            0.0   \n",
            "3  on the ncdc directly contacted a twitter user ...     0.0            0.0   \n",
            "4  indias confirmed case count crosses ten lakh m...     0.0            0.0   \n",
            "\n",
            "   aamir  aampe  abandon  abbott  abia  ...  zealand  zealanders  zealands  \\\n",
            "0    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "1    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "2    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "3    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "4    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "\n",
            "   zero  zika  zinc  zithromax  zone  zones  zoom  \n",
            "0   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "1   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "2   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "3   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "4   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "\n",
            "[5 rows x 5003 columns]\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features\n",
        "data['processed_tweet'] = data['tweet'].apply(preprocess_text) # Applying preprocessing on data DataFrame\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['processed_tweet'])\n",
        "\n",
        "# Create a DataFrame for the TF-IDF matrix\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Concatenate the TF-IDF features with the original data\n",
        "data = pd.concat([data, tfidf_df], axis=1)\n",
        "\n",
        "# Display the first 5 rows with TF-IDF features\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT_p55lLYM1g",
        "outputId": "92d4fe6c-cb9b-44d7-fc6e-00562d4f0742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.90      0.90       899\n",
            "           1       1.00      1.00      1.00      1696\n",
            "\n",
            "   micro avg       0.97      0.96      0.97      2595\n",
            "   macro avg       0.96      0.95      0.95      2595\n",
            "weighted avg       0.97      0.96      0.97      2595\n",
            " samples avg       0.98      0.97      0.97      2595\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for KNN\n",
        "X = tfidf_matrix\n",
        "y = data['label'].map(lambda x: 0 if x == 'fake' else 1)  # Assuming 'label' column contains 'fake' or 'real'\n",
        "\n",
        "# Split data into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# KNN Model Training\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Y5mB11krOX"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for KNN using GridSearchCV\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='f1_macro') # Use 5-fold cross-validation\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best F1 score:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_knn_classifier = grid_search.best_estimator_\n",
        "y_pred = best_knn_classifier.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OySfs47ijaC3"
      },
      "outputs": [],
      "source": [
        "# Prepare data for Logistic Regression\n",
        "X = tfidf_matrix\n",
        "y = train_df['label'].map(lambda x: 0 if x == 'fake' else 1)  # Assuming 'label' column contains 'fake' or 'real'\n",
        "\n",
        "# Convert y to a 1D NumPy array to avoid the ValueError\n",
        "y = y.values.ravel()\n",
        "\n",
        "# Split data into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression Model Training\n",
        "logistic_regression_classifier = LogisticRegression(max_iter=1000) # Increased max_iter for converegence\n",
        "logistic_regression_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_regression_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dimaOboElR4R"
      },
      "outputs": [],
      "source": [
        "# LinearSVC Model Training\n",
        "linear_svc_classifier = LinearSVC()\n",
        "linear_svc_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = linear_svc_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G0Y0Oeyuhwp0"
      },
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['processed_tweet'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wiU1E866pa4I"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ju25HSgEuHNu",
        "outputId": "40407964-25e6-45de-cc6d-62f7a212fe18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               tweet label  \\\n",
            "0  #IndiaFightsCorona: India reports more than 90...  real   \n",
            "1  The continuing good news is that daily reporte...  real   \n",
            "2  • 3 people are currently in hospital one each ...  real   \n",
            "3  On the 15/03 NCDC directly contacted a Twitter...  real   \n",
            "4  India’s confirmed case count crosses ten lakh ...  fake   \n",
            "\n",
            "                                     processed_tweet  aajtak  aamaadmiparty  \\\n",
            "0  indiafightscorona india reports more than reco...     0.0            0.0   \n",
            "1  the continuing good news is that daily reporte...     0.0            0.0   \n",
            "2  people are currently in hospital one each in a...     0.0            0.0   \n",
            "3  on the ncdc directly contacted a twitter user ...     0.0            0.0   \n",
            "4  indias confirmed case count crosses ten lakh m...     0.0            0.0   \n",
            "\n",
            "   aamir  aampe  abandon  abbott  abia  ...  zealand  zealanders  zealands  \\\n",
            "0    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "1    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "2    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "3    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "4    0.0    0.0      0.0     0.0   0.0  ...      0.0         0.0       0.0   \n",
            "\n",
            "   zero  zika  zinc  zithromax  zone  zones  zoom  \n",
            "0   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "1   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "2   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "3   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "4   0.0   0.0   0.0        0.0   0.0    0.0   0.0  \n",
            "\n",
            "[5 rows x 5003 columns]\n"
          ]
        }
      ],
      "source": [
        "# KMeans Clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)  # Assuming 2 clusters for fake and real\n",
        "kmeans.fit(tfidf_matrix)\n",
        "\n",
        "# Get Cluster labels for each data point\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Add cluster labels to the DataFrame\n",
        "data['cluster'] = cluster_labels\n",
        "\n",
        "# Display the first 5 rows with cluster labels\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lSCD1FKm3irS",
        "outputId": "bffb25fb-db07-4ed5-f348-5e8b668562f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KMeans Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.03      0.01      0.02      4029\n",
            "           1       0.45      0.72      0.55      4451\n",
            "\n",
            "    accuracy                           0.38      8480\n",
            "   macro avg       0.24      0.37      0.28      8480\n",
            "weighted avg       0.25      0.38      0.30      8480\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred_kmeans = data['cluster']\n",
        "print(\"KMeans Classification Report:\")\n",
        "print(classification_report(y, y_pred_kmeans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RsaCozRxxXP7"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "scFqf56GEirL",
        "outputId": "7d170f55-a684-4a07-94d4-37f74827b68e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural Network Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.91       797\n",
            "           1       0.93      0.92      0.92       899\n",
            "\n",
            "    accuracy                           0.92      1696\n",
            "   macro avg       0.92      0.92      0.92      1696\n",
            "weighted avg       0.92      0.92      0.92      1696\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Neural Network Model Training\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
        "mlp_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_mlp = mlp_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Neural Network Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_mlp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Entz1G40hf"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting Model Training\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_gb = gb_classifier.predict(X_test)\n",
        "\n",
        "# Evalauate the model\n",
        "print(\"Gradient Boosting Classification Report\")\n",
        "print(classification_report(y_test, y_pred_gb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wXdOUI5Ld54"
      },
      "source": [
        "Implementation of pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzhstfVUK8t7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define activation functions\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return torch.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "  return torch.relu(x)\n",
        "\n",
        "def leaky_relu(x, negative_slope=0.01):\n",
        "  return torch.nn.functional.leaky_relu(x, negative_slope=negative_slope)\n",
        "\n",
        "# Example usage:\n",
        "x = torch.tensor([-1.0, 0.0, 1.0])\n",
        "\n",
        "print(\"Sigmoid:\", sigmoid(x))\n",
        "print(\"Tanh:\", tanh(x))\n",
        "print(\"ReLU:\", relu(x))\n",
        "print(\"Leaky ReLU:\", leaky_relu(x))\n",
        "\n",
        "\n",
        "# Using PyTorch's built-in activation functions (more efficient)\n",
        "sigmoid_activation = nn.Sigmoid()\n",
        "tanh_activation = nn.Tanh()\n",
        "relu_activation = nn.ReLU()\n",
        "leaky_relu_activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "print(\"\\nUsing PyTorch built-in functions:\")\n",
        "print(\"Sigmoid:\", sigmoid_activation(x))\n",
        "print(\"Tanh:\", tanh_activation(x))\n",
        "print(\"ReLU:\", relu_activation(x))\n",
        "print(\"Leaky ReLU:\", leaky_relu_activation(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffHGRqDzP1NI"
      },
      "outputs": [],
      "source": [
        "# Define the cost function (loss function)\n",
        "def cost_function(y_pred, y_true):\n",
        "    mse_loss = nn.MSELoss()  # Use PyTorch's built-in MSE loss function\n",
        "    return mse_loss(y_pred, y_true)\n",
        "\n",
        "# Example usage:\n",
        "y_predicted = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y_actual = torch.tensor([1.5, 1.8, 3.2])\n",
        "loss = cost_function(y_predicted, y_actual)\n",
        "\n",
        "print(\"MSE loss:\", loss)\n",
        "\n",
        "\n",
        "# Example using binary cross-entropy loss\n",
        "def binary_cross_entropy_cost(y_pred, y_true):\n",
        "    bce_loss = nn.BCELoss()\n",
        "    return bce_loss(y_pred, y_true)\n",
        "\n",
        "# Example usage (make sure y_pred is between 0 and 1, e.g., use a sigmoid activation)\n",
        "y_predicted_prob = torch.sigmoid(torch.tensor([1.0, 2.0, 3.0], requires_grad=True))  # Example: Using sigmoid to get probabilities\n",
        "y_actual_binary = torch.tensor([1.0, 0.0, 1.0]) #Example binary target labels\n",
        "loss = binary_cross_entropy_cost(y_predicted_prob, y_actual_binary)\n",
        "print(\"Binary Cross-Entropy loss:\", loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm2p5l9OVm6z"
      },
      "outputs": [],
      "source": [
        "\n",
        "data['new_attribute'] = 0\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjladZzWWnW3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Assuming 'data' DataFrame and 'tfidf_matrix' are already defined as in your code.\n",
        "\n",
        "# Convert the TF-IDF matrix to a PyTorch tensor\n",
        "X_tensor = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Convert the labels to a PyTorch tensor\n",
        "# Instead of y.values, directly use the label column from the DataFrame.\n",
        "y_tensor = torch.tensor(data['label'].map(lambda x: 0 if x == 'fake' else 1).values, dtype=torch.long) # Use torch.long for classification labels\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"X_tensor shape:\", X_tensor.shape)\n",
        "print(\"y_tensor shape:\", y_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS_PKuiZZSiS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Assuming 'data' DataFrame and 'tfidf_matrix' are already defined as in your code.\n",
        "\n",
        "# Convert the TF-IDF matrix to a PyTorch tensor\n",
        "X_tensor = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Convert the labels to a PyTorch tensor\n",
        "# Instead of y.values, directly use the label column from the DataFrame.\n",
        "y_tensor = torch.tensor(data['label'].map(lambda x: 0 if x == 'fake' else 1).values, dtype=torch.long) # Use torch.long for classification labels\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"X_tensor shape:\", X_tensor.shape)\n",
        "print(\"y_tensor shape:\", y_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st8auWZcakRO"
      },
      "outputs": [],
      "source": [
        "# Add dropout regularization to the model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate) # Dropout layer\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x) # Apply dropout\n",
        "        x = self.fc2(x)\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9DhSi1ajBWF"
      },
      "outputs": [],
      "source": [
        "# Define the neural network model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_tensor.shape[1]  # Number of features\n",
        "hidden_size = 64  # Number of neurons in the hidden layer\n",
        "output_size = 2  # Number of output classes (fake/real)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "# Changed the criterion to BCELoss, since output is not one-hot encoded\n",
        "criterion = nn.BCELoss()  # Use cross-entropy loss for classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_tensor)\n",
        "    # Apply sigmoid for probabilities\n",
        "    outputs = torch.sigmoid(outputs)\n",
        "    loss = criterion(outputs, y_tensor.float()) # The target should be class probabilities (FloatTensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Make predictions on the entire dataset (for demonstration)\n",
        "with torch.no_grad():  # No need to track gradients during prediction\n",
        "    predicted_probabilities = torch.sigmoid(model(X_tensor))  # Get probabilities\n",
        "    predicted_labels = (predicted_probabilities >= 0.5).int()  # Get predicted class labels (0 or 1)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_tensor.cpu(), predicted_labels.cpu())) # Move tensors to CPU for sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7V5bFSgH3R5"
      },
      "outputs": [],
      "source": [
        "class RNN2DFlatten(nn.Module):\n",
        "    def __init__(self, input_height, input_width, hidden_size, num_layers, num_classes):\n",
        "        super(RNN2DFlatten, self).__init__()\n",
        "        self.input_height = input_height\n",
        "        self.input_width = input_width\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Flatten 2D input to 1D (height * width)\n",
        "        self.rnn = nn.RNN(input_height * input_width, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, height, width)\n",
        "        batch_size = x.size(0)\n",
        "        seq_length = x.size(1)\n",
        "\n",
        "        # Flatten spatial dimensions\n",
        "        x = x.view(batch_size, seq_length, -1)\n",
        "\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B82J0wxtmLz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        output = self.fc(output[:, -1, :])  # Take last time step's output\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "\n",
        "# Example usage for text classification\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, char_to_idx, max_length):\n",
        "        self.texts = [self.text_to_tensor(text, char_to_idx, max_length) for text in texts]\n",
        "        self.labels = labels\n",
        "\n",
        "    def text_to_tensor(self, text, char_to_idx, max_length):\n",
        "        indices = [char_to_idx[char] for char in text[:max_length]]\n",
        "        if len(indices) < max_length:\n",
        "            indices += [0] * (max_length - len(indices))  # Padding\n",
        "        return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Training loop\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        hidden = model.init_hidden(inputs.size(0)).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(inputs, hidden)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlefcDllyOT3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(model, start_seq, char_to_idx, idx_to_char, length=100, temperature=1.0, device='cpu'):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[c] for c in start_seq], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1).to(device)\n",
        "\n",
        "    generated_seq = start_seq\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(start_seq) - 1):\n",
        "            _, hidden = model(input_seq[:, i].unsqueeze(1), hidden)\n",
        "\n",
        "        last_char = input_seq[:, -1].unsqueeze(1)\n",
        "\n",
        "        for _ in range(length):\n",
        "            output, hidden = model(last_char, hidden)  # output shape: (1, output_size)\n",
        "            output_dist = output.data.view(-1).div(temperature).exp()\n",
        "            top_char = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "            predicted_char = idx_to_char[top_char.item()]\n",
        "            generated_seq += predicted_char\n",
        "\n",
        "            last_char = torch.tensor([[top_char]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated_seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iOLuQGJxaED"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM2D(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM2D, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfnnTy5yxwzd"
      },
      "outputs": [],
      "source": [
        "# Example parameters\n",
        "batch_size = 4\n",
        "seq_length = 10\n",
        "input_size = 8\n",
        "hidden_size = 16\n",
        "num_layers = 2\n",
        "num_classes = 3\n",
        "\n",
        "# Create model\n",
        "model = LSTM2D(input_size, hidden_size, num_layers, num_classes)\n",
        "\n",
        "# Dummy input: batch of sequences\n",
        "x = torch.randn(batch_size, seq_length, input_size)\n",
        "\n",
        "# Forward pass\n",
        "output = model(x)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Output tensor:\", output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}